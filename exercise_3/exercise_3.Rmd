---
title: "Jayme_Blake_Exercise3"
author: "Jayme Gerring and Blake (Pin-Yun) Lin"
date: "3/24/2022"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## hint r script: glass, tree_example, random_forest, hockey

library(tidyverse)
library(rpart)
library(rpart.plot)
library(rsample) 
library(randomForest)
library(lubridate)
library(modelr)
library(gbm)
library(caret)
library(ggmap)
library(maps)
library(mapdata)

greenbuildings = read.csv('../data/greenbuildings.csv', header=TRUE)
CAhousing = read.csv('../data/CAhousing.csv', header=TRUE)
dengue = read.csv('../data/dengue.csv', header=TRUE)

# override the default setting of ggplot
theme_set(theme_minimal())

```
## Problem 1: What Causes What?

1. There's a causality problem, it's hard to come to a clear conclusion just by looking at police force size. Cities with higher than average crime rate might hire more police than an average city, this might lead to a false conclusion that police are ineffective at solving crime. On the other hand, having more police means that crime gets more easily detected, this might lead someone to conclude crime rates are higher when in fact that might be the same as any other city with a smaller police force. Simply put, it's hard to conclude what effect increased policing has on crime. 

2. Basically, the researchers added an IV variable by using the terror alert system. High Terror alert means that there will be an increased police presence regardless of the amount of crime that's happening in a given area. In their first regression they found that a high terror alert is predicted to lower the number of crime by about 7 crimes. In the second regression, controlling for metro ridership, the high terror alert is predicted to lower the crime rate by about 6 crimes.\

3. The researchers decided to control for metro ridership to make sure that the lower crime rate caused by the high terror rate wasn't simply a matter of a smaller amount of people being out and about on the street. The researchers were trying to capture the effect that a high terror rate could have on the amount of people in the city. 

4. The first column comprises of a linear model using robust regression with three coefficients. One of the coefficients looks at the effect of a high terror rate solely within the first police district area, meaning the national mall. This is because if terrorists were to attack Washington, D.C. they would probably focus on this area. The next coefficient is the effect of the high alert on the rest of the police district areas within DC. The third coefficient is the log of midday metro ridership. Basically this regression is showing that the high alert (and therefore an increased number of police) lowers crime mostly in the National Mall area, the effect in the rest of the city isn't as profound as it is in the other area, even though it still lowers crime by a small amount. However, the regression still shows strong evidence that more police lowers crime, this is because during a high alert the DC police force is probably going to increase police the most in district one. 

## Problem 2 Tree Modeling: Dengue Cases
### Part 1: CART

```{r 2, message=FALSE, echo=FALSE}

#fixing na values 
dengue <- na.exclude(dengue)
dengue$city = dengue$city %>% factor()
dengue$season = dengue$season %>% factor()

#create a testing and training set
dengue_split = initial_split(dengue, prop = 0.9)
dengue_train = training(dengue_split)
dengue_test = testing(dengue_split)

#creating the tree, CART model

dengue_tree = rpart(total_cases ~ ., data = dengue_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

rpart.plot(dengue_tree, digits=-5, type=4, extra=1)
```

The model above shows the un-pruned CART Tree, we will proceed to prune and then calculate RMSE.


``` {r 2 cont , message=FALSE, echo=FALSE}
# this function actually prunes the tree at that level
prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

#lets prune to make sure we have the best model
prune_dengue_tree = prune_1se(dengue_tree)

#checking
rmse_CART = rmse(prune_dengue_tree, dengue_test)

cat(rmse_CART,' RMSE for Pruned CART Model') 

```

### Part 2: Random Forest
``` {r 2 cont again , message=FALSE, echo=FALSE}
#random forest

DengueRandom = randomForest(total_cases ~ ., data= dengue_train, importance = TRUE)

plot(DengueRandom)


```

This plot shows the out of bag MSE as a function of the number of trees used. Let's proceed to look at the RMSE compared to the testing set.


``` {r 2 random conclusion, message=FALSE, echo=FALSE}

rmse_random = rmse(DengueRandom, dengue_test)

cat(rmse_random,' RMSE for Random Forest')
```

### Part 3: Gradient Boosted Trees

``` {r 2 boosted , message=FALSE, echo=FALSE}

#boosted trees
DengueBoost = gbm(total_cases ~ ., data= dengue_train,
             interaction.depth=4, n.trees=350, shrinkage=.05, cv.folds = 10, 
             distribution='gaussian')

gbm.perf(DengueBoost)

```

This plot shows the error curve of the Gradient Boosted Model, with the optimal number of trees listed as output. Let's now check the RMSE for the Gradient Boosted Trees Model. 


``` {r 2 boosted conclusion, message=FALSE, echo=FALSE}

#checking
rmse_boosted = rmse(DengueBoost, dengue_test) 

cat(rmse_boosted,' RMSE for Gradient Boosted Trees') 

```

Looking at the RMSE results from the three models, it appears that random forest would be the best choice for this particular set of data. The next section shows the partial dependency plots for the Random Forest Model. 

### Part 4: Partial Dependency Plots
``` {r 2 PD plots, message=FALSE, echo=FALSE}
#pd plots
partialPlot(DengueRandom, dengue_test, 'specific_humidity', las=1)

partialPlot(DengueRandom, dengue_test, 'precipitation_amt', las=1)

partialPlot(DengueRandom, dengue_test, 'tdtr_k', las=1)
```

### Wrap Up: 

Looking at the PD plots, most seem to make sense in the context of the science of mosquito breeding. Mosquitos require standing water in order to make baby mosquitos, it makes sense that as precipitation increases, the number of mosquitos increases, the increased number of mosquitos leads to more cases of Dengue. The same seems to be true of humidity. Humidity is a measure of how much evaporated moisture there is in the air, higher humidity would seem to indicate that there is a higher amount of water on the ground, and thus the amount of mosquito breeding grounds. Our wild card PD plot looks at the Average Diurnal Temperature Range. It shows that as DTR increases, the amount of predicted Dengue cases decreases. This makes sense as well, it's possible that temperature shocks kill mosquitos which leads to less Dengue cases. 






```{r Q3 - CART, include=FALSE, echo=FALSE}
# create the revenue per per square foot variable 
greenbuildings = mutate(greenbuildings, revenue = Rent * leasing_rate)
greenbuildings = greenbuildings %>% drop_na()

# split data into training and testing
set.seed(100)
green_split =  initial_split(greenbuildings, prop=0.8)
green_train = training(green_split)
green_test  = testing(green_split)

# let's fit a single tree
green.tree = rpart(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07 - leasing_rate - Rent, data=green_train, control = rpart.control(cp = 0.00001), na.action=na.omit)
```


```{r Q3 - random forest, include=FALSE}
# now a random forest
green.forest = randomForest(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, data=green_train, na.action=na.omit, importance = TRUE)

```


```{r Q3 - gbm tuning for green, include=FALSE}

hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)


for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  green.gbm.tune <- gbm(
    revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, 
    data = green_train,
    distribution = "gaussian",
    n.trees = 500,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(green.gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(green.gbm.tune$valid.error))
}


# check which parameters are performing better 
top10_green = hyper_grid %>% 
              arrange(min_RMSE) %>%
              head(10)

# Then use this new grid 2 to run the loop again
# grid 2 
hyper_grid <- expand.grid(
  shrinkage = c(.05, .1, .2),
  interaction.depth = c(12, 15, 17),
  n.minobsinnode = c(3, 5, 10),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,              
  min_RMSE = 0                     
)

# decided on these parameters, and fit the final bgm
green.boost = gbm(revenue ~ . - LEED - Energystar - cd_total_07 - hd_total07- leasing_rate - Rent, data=green_train, interaction.depth=18, n.trees=600, shrinkage=.2, cv.folds = 10)

# compare RMSE 
rmse_green.tree=rmse(green.tree, green_test)
rmse_green.forest=rmse(green.forest, green_test) 
rmse_green.boost=rmse(green.boost, green_test) 
```



## Problem 3: Green Certification
### Introduction
This question asks us to quantify the effect of green certifications on revenue per square foot in buildings with such a certification. Green certifications clearly have an environmental impact, but do they also make buildings more attractive to potential renters, and people pay attention when buidlings recieve a green certification? We attempt to answer these concerns below. 

### Analysis
#### Data Cleaning 
Since we focused on the revenue of the property, we generated a new variable `revenue` as the product of `Rent` and `leasing_rate`. We created green_rating to see the overall impact of green certificate instead of using both  `LEED` and `EnergyStar` (`green_rating` is a collapsed version of these two ratings). Other important factors that will affect energy usage are the cooling degree days and the heating degree days, in our model we used `total_dd_07` to represent this factor.

#### Modeling
We tried three different models to see which one will yield the best out-of-sample RMSE, they are  CART, random forest, and gradient-boosted model. We split the data set into training and testing sets, and we trained all of the three models on training data using all of the variables.

To tune the gradient-boosted model, we first created a grid that specifies `shrinkage`, `interaction.depth`, `n.minobsinnode`, and `bag.fraction`. then ran the gbm  across all different combination of these parameters. After narrowing the tuning grid  2 times, the average RMSE across 10 folds is still higher than random forest. The final comparison are shown below.


#### Model performacne
The out-of-sample RMSE of the models:
CART: 772.38
Random Forest: 609.07
Gradient-boosted: 683.56

So we decided to use random forest as our best predictive model.


#### Plots
This is the variable importance plot for our random forest model. As you can see, size, market rent, and age seem to be the biggest factors in predicting. Our green rating variable is actually show to have the lowest importance out of all of the parameters. 
```{r Q3 - VI plot}
# variable importance measures
vi = varImpPlot(green.forest, type=1)

```

As you can see, it appears that green rating is only estimated to increase revenue by fifty dollars.
```{r partail plot, include=FALSE}
# partial dependence plots
# these are trying to isolate the partial effect of specific features
# on the outcome
pdp::partial(green.forest ,pred.var = "green_rating") %>% 
  ggplot() +
    geom_col(aes(x = factor(green_rating), y=yhat, fill = factor(green_rating))) +
    labs(x = "Green Certification",y = "Predicted Value", title = "Partial dependence plot of Green Certification")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
          guides(color = guide_legend(title=" Green Certification"))
```


### Wrap up
After testing three models, we decided to employ random forest. Using our predictive modeling, we found that green certification doesn’t really lead to a dramatic increase in revenue. According to our partial dependence plot, a green certification is only expected to increase yearly rent revenue (per square foot) by fifty dollars. 




```{r Q4 - plot 1 + tree and forest, include=FALSE}

# split data into training and testing:    
set.seed(101)
ca_split =  initial_split(CAhousing, prop=0.8)
ca_train = training(ca_split)
ca_test  = testing(ca_split)

# fit a single tree
ca.tree = rpart(medianHouseValue ~ . , data=ca_train, control = rpart.control(cp = 0.00001))

# random forest 
ca.forest = randomForest(medianHouseValue ~ . , data=ca_train, control = rpart.control(cp = 0.00001), importance=TRUE)

```


```{r Q4 - ca.bgm tuning, include=FALSE}

hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3),
  interaction.depth = c(1, 3, 5),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = medianHouseValue ~ .,
    data = ca_train,
    distribution = "gaussian",
    n.trees = 700,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

# check which parameters are performing better 
hyper_grid %>% 
  arrange(min_RMSE) %>%
  head(10)

# narrow the grid: second try
hyper_grid <- expand.grid(
  shrinkage = c(.1, .3, .5),
  interaction.depth = c(3, 5, 7),
  n.minobsinnode = c(5, 10, 15),
  bag.fraction = c(.65, .8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

# final boosted model
ca.boost = gbm(medianHouseValue ~ ., data = ca_train, distribution = "gaussian", interaction.depth=5, n.trees=659,  shrinkage=.3, cv.folds =10)

```




```{r Q4 compare three models, echo=FALSE, message=FALSE}
# the model we choose here is: random forest for now 
rmse_ca.tree = rmse(ca.tree, ca_test)
rmse_ca.forest = rmse(ca.forest, ca_test)
rmse_ca.boost = rmse(ca.boost, ca_test)
```



```{r Q4 - plots, include=FALSE}

# getting the California data
states <- map_data("state")
ca_df <- subset(states, region == "california")

# plain map of ca
ca_base <- ggplot(data = ca_df, mapping = aes(x = long, y = lat)) + 
  coord_fixed(1.3) + 
  geom_polygon(color = "black", fill = "gray")


# PLOT1: original data
ca_plot1 <- ca_base + geom_point(data = CAhousing, aes(x=longitude, y=latitude,    color=medianHouseValue))+scale_color_continuous(type = "viridis")+
          labs(title = " Actual Median House Value in California", x="longitude", y="latitude")+
          theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
          guides(color = guide_legend(title="Median Value"))


# PLOT2: prediction 
CAhousing = CAhousing %>%
  mutate(ca_pred = predict(ca.boost, CAhousing))

ca_plot2 <- ca_base + geom_point(data = CAhousing, aes(x=longitude, y=latitude, color=ca_pred)) + 
            scale_color_continuous(type = "viridis")+
            labs(title = "Predicted Median House Value in California", x="longitude", y="latitude")+
            theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
            guides(color = guide_legend(title=" Predicted Value"))

# PLOT3: residual
CAhousing = CAhousing %>%
  mutate(ca_resid = sqrt((medianHouseValue-ca_pred)^2))

ca_plot3 <- ca_base +
            geom_point(data = CAhousing, aes(x=longitude, y=latitude, color=ca_resid)) + 
            scale_color_continuous(type = "viridis")+
            labs(title = "Residuals of Median House Value in California", x="longitude", y="latitude")+
            theme(plot.title = element_text(hjust = 0.5, face = "bold"))+
            guides(color = guide_legend(title="Residual"))

```

## Problem 4: California Housing
#### Modeling
Again in this question we will compare CART, random forest, and gbm to get the best predictive model.
After tuning the gbm model, we get a out-of-sample RMSE that is smaller than random forest.
So the best predictive model we will use here would be gbm.

Now, we use the gbm model to produce a plot of prediction and a plot of model's residuals.

#### Plots
To produce the three plots, first we set up the base plot of California using the data from the package `maps`.

The plot displays the median house value in California with the colors becoming brighter as the house value increases. The plot clearly displays the higher home values of the Los Angeles and San Francisco Bay Area (including coastal suburbs), having the most concentrated collection of homes with high values. 

```{r Q4 - plot 1, include=FALSE}
ca_plot1
```
This plot displays the predicted median house value from our model. As you can see, compared to the actual data, our model appears to do a good job at predicting house value. 

```{r Q4 - plot 2, include=FALSE}

ca_plot2

```
This plot shows the absolute value of the residuals between actual and predicted values. It appears that most of the errors from our model are small. 
```{r Q4 - plot 3, include=FALSE}

ca_plot3

```

